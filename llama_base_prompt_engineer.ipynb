{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff91bc18-e30e-48f8-9c3f-0e5b1e6272cc",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17fdae32-025b-4d30-aa2f-6d0bc5bf6ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate==0.29.3 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (0.29.3)\n",
      "Requirement already satisfied: bitsandbytes==0.43.1 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (0.43.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from accelerate==0.29.3) (0.4.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from accelerate==0.29.3) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from accelerate==0.29.3) (2.3.0)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from accelerate==0.29.3) (0.23.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from accelerate==0.29.3) (5.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from accelerate==0.29.3) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from accelerate==0.29.3) (23.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from torch>=1.10.0->accelerate==0.29.3) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from torch>=1.10.0->accelerate==0.29.3) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from torch>=1.10.0->accelerate==0.29.3) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from torch>=1.10.0->accelerate==0.29.3) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from torch>=1.10.0->accelerate==0.29.3) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from torch>=1.10.0->accelerate==0.29.3) (2024.5.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from torch>=1.10.0->accelerate==0.29.3) (2021.4.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from huggingface-hub->accelerate==0.29.3) (4.65.0)\n",
      "Requirement already satisfied: requests in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from huggingface-hub->accelerate==0.29.3) (2.31.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate==0.29.3) (2021.12.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate==0.29.3) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate==0.29.3) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate==0.29.3) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.29.3) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.29.3) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.29.3) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.29.3) (1.26.18)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate==0.29.3) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate==0.29.3 bitsandbytes==0.43.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a360618-949f-4007-a630-636cb1e04881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28631c2a-400c-4846-92ad-3c99f6c8c97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free(Gb): 10.708058112 total(Gb): 11.81089792\n"
     ]
    }
   ],
   "source": [
    "# Check GPU usage:\n",
    "print(\"free(Gb):\", torch.cuda.mem_get_info()[0]/1000000000, \"total(Gb):\", torch.cuda.mem_get_info()[1]/1000000000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b6f81e-19c6-41de-ad6a-1bfa047dacbc",
   "metadata": {},
   "source": [
    "# Load Model and Tokenizer\n",
    "- We can write a wrapper function to load and generate text\n",
    "- load_in_8bit = True would speed up the inference significantly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ed84a4b-f340-4cac-9732-14b477c0a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get token from your huggingface page\n",
    "token = \"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\"\n",
    "llama = \"meta-llama/Llama-2-7b-hf\"\n",
    "load_in_8bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "536d5a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:769: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc2d4e0405c4e3e80c877610f582135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(llama,\n",
    "                                          use_auth_token=token)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llama,\n",
    "    use_auth_token=token,\n",
    "    device_map='auto',\n",
    "    load_in_8bit=load_in_8bit,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96546b4d-ef32-4652-b35e-1cbc04be7fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free(Gb): 3.1981568 total(Gb): 11.81089792\n"
     ]
    }
   ],
   "source": [
    "# Check gpu usage\n",
    "print(\"free(Gb):\", torch.cuda.mem_get_info()[0]/1000000000, \"total(Gb):\", torch.cuda.mem_get_info()[1]/1000000000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff37e90-fbbf-4f58-a6a0-851a01b96226",
   "metadata": {},
   "source": [
    "# Prompt Overview\n",
    "\n",
    "- A prompt contains any of the following elements:\n",
    "\n",
    "    - Instruction - a specific task or instruction you want the model to perform\n",
    "\n",
    "    - Context - external information or additional context that can steer the model to better responses\n",
    "\n",
    "    - Input Data - the input or question that we are interested to find a response for\n",
    "\n",
    "    - Output Indicator - the type or format of the output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3716f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c347b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> What is the sentiment of:\n",
      "Hi Amit, thanks for the thoughtful birthday card!\n",
      "Hi Amit, thanks for the birthday card!\n",
      "Hi Amit, thanks for the birthday card.\n",
      "Hi Amit, thanks for the birthday card.\n",
      "Hi Amit, thanks for the birthday card.\n",
      "Hi Amit, thanks for the birthday card.\n",
      "Hi Amit, thanks for the birthday card.\n",
      "Hi Amit, thanks for the birthday card.\n",
      "Hi Amit, thanks for the birthday card.\n",
      "Hi Amit\n"
     ]
    }
   ],
   "source": [
    "# Example not following the prompt elements!\n",
    "prompt = \"\"\"What is the sentiment of:\n",
    "Hi Amit, thanks for the thoughtful birthday card!\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device)\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], \n",
    "                         attention_mask=inputs[\"attention_mask\"],\n",
    "                         max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1eac112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Classify the text into neutral, negative or positive:\n",
      "Text: Hi Amit, thanks for the thoughtful birthday card!\n",
      "Sentiment: Neutral\n",
      "Text: This is the worst birthday card ever!\n",
      "Sentiment: Negative\n",
      "Text: Happy birthday! I hope you have a great day!\n",
      "Text: I hope you have a great birthday!\n",
      "Text: Happy birthday!\n",
      "Text: I hope you have a great birthday!\n",
      "Text: Happy birthday! I hope you have a great day!\n",
      "Text: Happy birthday! I hope you have a great day!\n",
      "Text\n"
     ]
    }
   ],
   "source": [
    "# Example following the prompt elements!\n",
    "\n",
    "prompt = \"\"\"Classify the text into neutral, negative or positive:\n",
    "Text: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment:\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device)\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], \n",
    "                         attention_mask=inputs[\"attention_mask\"],\n",
    "                         max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65d482ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Classify the text into neutral, negative or positive:\n",
      "Text: Hi Amit, thanks for the thoughtful birthday card!\n",
      "Sentiment: 0.00%\n",
      "Sentiment: 0.00%\n",
      "Text: Hi Amit, thanks for the thoughtful birthday card!\n",
      "Sentiment: 0.00%\n",
      "Sentiment: 0.00%\n",
      "Text: Hi Amit, thanks for the thoughtful birthday card!\n",
      "Sentiment: 0.00%\n",
      "Sentiment: 0.00%\n",
      "Text: Hi Amit, thanks\n"
     ]
    }
   ],
   "source": [
    "# Example following the prompt elements!\n",
    "\n",
    "prompt = \"\"\"Classify the text into neutral, negative or positive:\n",
    "Text: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment: \"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device)\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], \n",
    "                         attention_mask=inputs[\"attention_mask\"],\n",
    "                         max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42249e3a",
   "metadata": {},
   "source": [
    "### One-shot Prompting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cf2824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example following the prompt elements!\n",
    "\n",
    "prompt = \"\"\"Classify the text into neutral, negative or positive:\n",
    "Text: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment:\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device)\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], \n",
    "                         attention_mask=inputs[\"attention_mask\"],\n",
    "                         max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfe549d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dc8d172",
   "metadata": {},
   "source": [
    "### Few-shot Prompting\n",
    "- 0x0A = new line token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2496b554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
      "Sentiment: Negative\n",
      "\n",
      "Message: Can't wait to order pizza for dinner tonight\n",
      "Sentiment: Positive\n",
      "\n",
      "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
      "Sentiment: Positive\n",
      "\n",
      "Message: I'm going to the beach for my vacation this weekend!\n",
      "Sentiment: Positive\n",
      "\n",
      "Message: I'm feeling a little sick today.\n",
      "Sentiment: Negative\n",
      "\n",
      "Message: I'm going to the beach for my vacation this weekend!\n",
      "Sentiment: Positive\n",
      "\n",
      "Message: I'm feeling a little sick today.\n",
      "Sentiment: Negative\n",
      "\n",
      "Message: I'\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment:\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device)\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], \n",
    "                         attention_mask=inputs[\"attention_mask\"],\n",
    "                         max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34091035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Message', ':', '▁Hi', '▁D', 'ad', ',', '▁you', \"'\", 're', '▁', '2', '0', '▁minutes', '▁late', '▁to', '▁my', '▁piano', '▁rec', 'ital', '!', '<0x0A>', 'S', 'ent', 'iment', ':', '▁Neg', 'ative', '<0x0A>', '<0x0A>', 'Message', ':', '▁Can', \"'\", 't', '▁wait', '▁to', '▁order', '▁p', 'izza', '▁for', '▁dinner', '▁ton', 'ight', '<0x0A>', 'S', 'ent', 'iment', ':', '▁Pos', 'itive', '<0x0A>', '<0x0A>', 'Message', ':', '▁Hi', '▁Am', 'it', ',', '▁thanks', '▁for', '▁the', '▁thought', 'ful', '▁birth', 'day', '▁card', '!', '<0x0A>', 'S', 'ent', 'iment', ':']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162be0ef",
   "metadata": {},
   "source": [
    "### Notice,new line at the begining and end does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6921bf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\" \n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment:\n",
    "\"\"\" \n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device)\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"],\n",
    "                         attention_mask=inputs[\"attention_mask\"],\n",
    "                         max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0]) print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3cb07b",
   "metadata": {},
   "source": [
    "### Notice, there should be no new lien at the end (at the begining is OK)\n",
    "### This is becuase we want to keep the sample format as the previous two demonstrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20ec6a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> \n",
      "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
      "Sentiment: Negative\n",
      "\n",
      "Message: Can't wait to order pizza for dinner tonight\n",
      "Sentiment: Positive\n",
      "\n",
      "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
      "Sentiment: Positive\n",
      "\n",
      "Message: You are my hero!\n",
      "Sentiment: Positive\n",
      "\n",
      "Message: I'm so proud of you!\n",
      "Sentiment: Positive\n",
      "\n",
      "Message: I'm so sorry about your grandmother's passing.\n",
      "Sentiment: Negative\n",
      "\n",
      "Message: I'm so sorry about your grandmother's passing.\n",
      "Sentiment: Negative\n",
      "\n",
      "Message: I'm so sorry about your grandm\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
    "Sentiment: Negative\n",
    "\n",
    "Message: Can't wait to order pizza for dinner tonight\n",
    "Sentiment: Positive\n",
    "\n",
    "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
    "Sentiment:\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device)\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], \n",
    "                         attention_mask=inputs[\"attention_mask\"],\n",
    "                         max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56c88b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '<0x0A>', 'Message', ':', '▁Hi', '▁D', 'ad', ',', '▁you', \"'\", 're', '▁', '2', '0', '▁minutes', '▁late', '▁to', '▁my', '▁piano', '▁rec', 'ital', '!', '<0x0A>', 'S', 'ent', 'iment', ':', '▁Neg', 'ative', '<0x0A>', '<0x0A>', 'Message', ':', '▁Can', \"'\", 't', '▁wait', '▁to', '▁order', '▁p', 'izza', '▁for', '▁dinner', '▁ton', 'ight', '<0x0A>', 'S', 'ent', 'iment', ':', '▁Pos', 'itive', '<0x0A>', '<0x0A>', 'Message', ':', '▁Hi', '▁Am', 'it', ',', '▁thanks', '▁for', '▁the', '▁thought', 'ful', '▁birth', 'day', '▁card', '!', '<0x0A>', 'S', 'ent', 'iment', ':']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8accf37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e2bb395-2a3e-4e9a-8b57-b17d55d7bc78",
   "metadata": {},
   "source": [
    "## Chain of Thought (CoT)\n",
    "\n",
    "Introduced in Wei et al. (2022), chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding.\n",
    "\n",
    "![image.png](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&w=1920&q=75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a681d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls.How many tennis balls does he have now? \n",
    "A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n",
    "A:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11775ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f425324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c64e7d4",
   "metadata": {},
   "source": [
    "### Few Shot CoT\n",
    "\n",
    "#### NOTICE! The resoning is incorrect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "368d4ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> \n",
      "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
      "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
      "\n",
      "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
      "A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n",
      "\n",
      "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
      "A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n",
      "\n",
      "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
      "A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\n",
      "\n",
      "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
      "A: Adding all the odd numbers (15, 32) gives 47. The answer is True.\n",
      "\n",
      "The odd numbers in this group add up to an even number: 1, 3, 5, 7, 9, 11, 13, 15, 17.\n",
      "A: Adding all the odd numbers (1, 3, 5, 7, 9, 11, 13,\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A:\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device)\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], \n",
    "                         attention_mask=inputs[\"attention_mask\"],\n",
    "                         max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47257e18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeab9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93380d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac8706ad",
   "metadata": {},
   "source": [
    "### One Shot CoT\n",
    "- Notice, in the original paper, even one-shot CoT worked!\n",
    "    - Keep in mind that the authors claim that this is an **emergent** ability that arises with **sufficiently large language models**.\n",
    "- Notice, the reasoning is incorrect\n",
    "\t- it seems like the mode ==does not tell the difference between even and odd number==\n",
    "\t\t- missed some odd number\n",
    "\t\t- incorrectly identify 32 as odd number\n",
    "\t\t- ==the addition is correct==, the sum of 15, 32, 5, 13, 82, 7, 1 is indeed 155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f201268b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> \n",
      "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
      "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
      "\n",
      "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
      "A: Adding all the odd numbers (15, 32, 5, 13, 82, 7, 1) gives 155. The answer is True.\n",
      "\n",
      "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
      "A: Adding all the odd numbers (15, 32, 5\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A:\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device)\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], \n",
    "                         attention_mask=inputs[\"attention_mask\"],\n",
    "                         max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0aa4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e3d32b2",
   "metadata": {},
   "source": [
    "#### Exmaple from Wei et al. 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "340fbb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> \n",
      "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? \n",
      "A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
      "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more. How many apples do they have?\n",
      "A: 23 apples - 20 apples = 3 apples. 23 - 20 = 3. The answer is 3.\n",
      "Q: 60000000000000000000000000000000000000000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? \n",
    "A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more. How many apples do they have?\n",
    "A:\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device)\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], \n",
    "                         attention_mask=inputs[\"attention_mask\"],\n",
    "                         max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623e229b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "289e26b3-4928-440b-b962-757fcd018456",
   "metadata": {},
   "source": [
    "### Zero Shot CoT\n",
    "\n",
    "One recent idea that came out more recently is the idea of zero-shot CoT (Kojima et al. 2022) that essentially involves adding **\"Let's think step by step\"** to the original prompt. Let's try a simple problem and see how the model performs:\n",
    "\n",
    "![image.png](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzero-cot.79793bee.png&w=1920&q=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3e54df",
   "metadata": {},
   "source": [
    "#### Without CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e90bbb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> \n",
      "I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1.\n",
      "How many apples did I remain with?\n",
      "\n",
      "### Solution\n",
      "\n",
      "The solution to the puzzle is 10.\n",
      "\n",
      "## Proof\n",
      "\n",
      "We note that the puzzle asks for the number of apples we remain with.\n",
      "\n",
      "Let $x$ be the number of apples we started with.\n",
      "\n",
      "Let $y$ be the number of apples we have left with.\n",
      "\n",
      "Let $z$ be the number of apples we have given away.\n",
      "\n",
      "We note that:\n",
      "\n",
      "$\\\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1.\n",
    "How many apples did I remain with?\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device)\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], \n",
    "                         attention_mask=inputs[\"attention_mask\"],\n",
    "                         max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864eeb6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd40e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db8144ef-7f8f-4a3f-b5bc-89899a04c7d9",
   "metadata": {},
   "source": [
    "### With CoT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "effa118a-d826-4255-a02a-538b649d4c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> \n",
      "I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
      "Let's think step by step.\n",
      " \n",
      "**Step 1:** I bought 10 apples.\n",
      " \n",
      "**Step 2:** I gave 2 apples to the neighbor and 2 to the repairman.\n",
      " \n",
      "**Step 3:** I then went and bought 5 more apples and ate 1.\n",
      "\n",
      "So, I remained with 5 apples.\n",
      "\n",
      "\n",
      "### Note:\n",
      "\n",
      "This question is asked in the interview of some companies like\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
    "Let's think step by step.\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device)\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], \n",
    "                         attention_mask=inputs[\"attention_mask\"],\n",
    "                         max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d58d5003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> \n",
      "Q: I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
      "A: Let's think step by step.\n",
      "\n",
      "- 10 apples\n",
      "- 2 apples to the neighbor and 2 to the repairman\n",
      "- 5 more apples and ate 1.\n",
      "\n",
      "10 - 2 - 2 = 6\n",
      "\n",
      "6 - 5 - 1 = 4\n",
      "\n",
      "4 - 1 = 3\n",
      "\n",
      "3 - 1 = 2\n",
      "\n",
      "2 - 1 = 1\n",
      "\n",
      "1 - 1 = 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Q: I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
    "A: Let's think step by step.\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device)\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], \n",
    "                         attention_mask=inputs[\"attention_mask\"],\n",
    "                         max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e9284efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> \n",
      "Q: I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples. How many apples did I remain with?\n",
      "A: Let's think step by step.\n",
      "\n",
      "**Step 1.** 10 apples - 2 apples = 8 apples\n",
      "\n",
      "**Step 2.** 8 apples - 2 apples = 6 apples\n",
      "\n",
      "**Step 3.** 6 apples - 2 apples = 4 apples\n",
      "\n",
      "**Step 4.** 4 apples - 2 apples = 2 apples\n",
      "\n",
      "**Step 5.** 2 app\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Q: I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples. How many apples did I remain with?\n",
    "A: Let's think step by step.\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device)\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], \n",
    "                         attention_mask=inputs[\"attention_mask\"],\n",
    "                         max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d660f9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> \n",
      "Q: I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman.I then went and bought 5 more apples. How many apples did I remain with?\n",
      "A: Let's think step by step.\n",
      "\n",
      "1. I bought 10 apples.\n",
      "2. I gave 2 apples to the neighbor and 2 to the repairman.\n",
      "3. I then went and bought 5 more apples.\n",
      "\n",
      "The total number of apples I bought is 10+5=15.\n",
      "\n",
      "The number of apples I have now is 15-2-2=11.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Q: I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples. How many apples did I remain with?\n",
    "A: Let's think step by step.\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device)\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], \n",
    "                         attention_mask=inputs[\"attention_mask\"],\n",
    "                         max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "13fa05c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> \n",
      "Q: I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples. How many apples did I remain with?\n",
      "A: Let's think step by step.\n",
      "\n",
      "1. I bought 10 apples.\n",
      "2. I gave 2 apples to the neighbor and 2 to the repairman.\n",
      "3. I then went and bought 5 more apples.\n",
      "\n",
      "I have 5 apples left.\n",
      "\n",
      "# 숫자 감소\n",
      "\n",
      "# 문제 출처\n",
      "\n",
      "[네이버 �������\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Q: I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples. How many apples did I remain with?\n",
    "A: Let's think step by step.\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device)\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], \n",
    "                         attention_mask=inputs[\"attention_mask\"],\n",
    "                         max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aea94177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> \n",
      "Explain each intermediate step.Only when you are done with all your steps, provide the answer based on your intermediate steps.\n",
      "Q: I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples. How many apples did I remain with?\n",
      "A: Let's think step by step.\n",
      "I bought 10 apples.\n",
      "I gave 2 apples to the neighbor and 2 to the repairman.\n",
      "I then went and bought 5 more apples.\n",
      "So now I have 10 + 2 + 2 + 5 = 19 apples.\n",
      "Now, I have 19 apples and I gave 2 apples to the neighbor and 2 to the repairman.\n",
      "So now I have 19 -\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Explain each intermediate step.Only when you are done with all your steps, provide the answer based on your intermediate steps.\n",
    "Q: I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples. How many apples did I remain with?\n",
    "A: Let's think step by step.\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device)\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], \n",
    "                         attention_mask=inputs[\"attention_mask\"],\n",
    "                         max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5af32b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
