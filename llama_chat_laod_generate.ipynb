{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff91bc18-e30e-48f8-9c3f-0e5b1e6272cc",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17fdae32-025b-4d30-aa2f-6d0bc5bf6ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate==0.29.3\n",
      "  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
      "     ---------------------------------------- 0.0/297.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 297.6/297.6 kB 9.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: bitsandbytes==0.43.1 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (0.43.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from accelerate==0.29.3) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from accelerate==0.29.3) (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from accelerate==0.29.3) (1.26.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from accelerate==0.29.3) (0.4.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from accelerate==0.29.3) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from accelerate==0.29.3) (0.23.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from accelerate==0.29.3) (23.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from torch>=1.10.0->accelerate==0.29.3) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from torch>=1.10.0->accelerate==0.29.3) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from torch>=1.10.0->accelerate==0.29.3) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from torch>=1.10.0->accelerate==0.29.3) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from torch>=1.10.0->accelerate==0.29.3) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from torch>=1.10.0->accelerate==0.29.3) (2024.5.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from torch>=1.10.0->accelerate==0.29.3) (2021.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from huggingface-hub->accelerate==0.29.3) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from huggingface-hub->accelerate==0.29.3) (4.65.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate==0.29.3) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate==0.29.3) (2021.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate==0.29.3) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate==0.29.3) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.29.3) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.29.3) (1.26.18)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.29.3) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from requests->huggingface-hub->accelerate==0.29.3) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate==0.29.3) (1.3.0)\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.30.1\n",
      "    Uninstalling accelerate-0.30.1:\n",
      "      Successfully uninstalled accelerate-0.30.1\n",
      "Successfully installed accelerate-0.29.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate==0.29.3 bitsandbytes==0.43.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a360618-949f-4007-a630-636cb1e04881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28631c2a-400c-4846-92ad-3c99f6c8c97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free(Gb): 10.708058112 total(Gb): 11.81089792\n"
     ]
    }
   ],
   "source": [
    "print(\"free(Gb):\", torch.cuda.mem_get_info()[0]/1000000000, \"total(Gb):\", torch.cuda.mem_get_info()[1]/1000000000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b6f81e-19c6-41de-ad6a-1bfa047dacbc",
   "metadata": {},
   "source": [
    "# Load Model and Tokenizer\n",
    "- Note, load_in_8bit = True would speed up the inference significantly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ed84a4b-f340-4cac-9732-14b477c0a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get token from your huggingface page\n",
    "token = \"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\"\n",
    "llama = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "load_in_8bit = True # The model get much faster with load_in_8bit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89e6b815-f5d6-42e5-ac79-957589380f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:769: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a521ddefb042f2b81ec3426d369c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(llama,\n",
    "                                          use_auth_token=token)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llama,\n",
    "    use_auth_token=token,\n",
    "    device_map='auto',\n",
    "    load_in_8bit=load_in_8bit,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96546b4d-ef32-4652-b35e-1cbc04be7fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free(Gb): 3.1981568 total(Gb): 11.81089792\n"
     ]
    }
   ],
   "source": [
    "# Check gpu usage\n",
    "print(\"free(Gb):\", torch.cuda.mem_get_info()[0]/1000000000, \"total(Gb):\", torch.cuda.mem_get_info()[1]/1000000000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff37e90-fbbf-4f58-a6a0-851a01b96226",
   "metadata": {},
   "source": [
    "# Tokenizer Overview\n",
    "\n",
    "The tokenizer breaks down the words into the common parts called tokens and then represents every token with the corresponding number.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6dcedd-3153-41f9-983a-3450a9384f8b",
   "metadata": {},
   "source": [
    "## From Words to Tokens\n",
    "\n",
    "- Notice how the word \"Llama\" get break down into 3 tokens: \"_L\", \"l\", \"ama\".\n",
    "\n",
    "- Notice how the number \"2\" get break down into 2 tokens: \"_\", \"2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d39a4a6-cd5d-451f-aec8-d69d345a308d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Meta', '▁developed', '▁and', '▁public', 'ly', '▁released', '▁the', '▁L', 'l', 'ama', '▁', '2', '▁family', '▁of', '▁large', '▁language', '▁models']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(tokenizer.tokenize('Meta developed and publicly released the Llama 2 family of large language models'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4310d4f1-23eb-4c79-ae31-d56b77576365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.tokenize('Meta developed and publicly released the Llama 2 family of large language models')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09fa731-9c19-430f-83e8-ec8ca204852e",
   "metadata": {},
   "source": [
    "## From Words to Numbers\n",
    "\n",
    "- Note that the tokenizer also provides us with the attention mask.\n",
    "    - The attention mask indicates whether the model should pay attention to the corresponding token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cff398fc-7001-4754-8b3e-9c3916132906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 20553, 8906, 322, 970, 368, 5492, 278, 365, 29880, 3304, 29871, 29906, 3942, 310, 2919, 4086, 4733], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Meta developed and publicly released the Llama 2 family of large language models'\n",
    "inputs = tokenizer(sentence)\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "797e020a-d8e4-4d5e-ba39-208c856cfa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2bb395-2a3e-4e9a-8b57-b17d55d7bc78",
   "metadata": {},
   "source": [
    "## From Numbers to Tokens\n",
    "\n",
    "- The tokens are perfectly reversible, and we can ask the tokenizer to convert the list of tokens back to the human language.\n",
    "- Notice the s token we get after reversing the tokenization. Where does it come from?\n",
    "    -  s is the symbol explaining Llama-2 that this is the beginning of the user input (begining of sequence token).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51a7946e-57bf-424e-820c-244f29f0eb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Meta developed and publicly released the Llama 2 family of large language models'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d78736f-c864-4fd1-96ec-1694b87ab25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e811ff0e-6b5d-4764-a2f2-3b462236e4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289e26b3-4928-440b-b962-757fcd018456",
   "metadata": {},
   "source": [
    "## Pad End of Sequence Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09281c0c-88b6-4851-aaaf-ede3168e067f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f1654c4-e8e1-4810-985d-cd94cc14c6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Meta developed and publicly released the Llama 2 family of large language models'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(llama,\n",
    "                                          use_auth_token=token)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "sentence = 'Meta developed and publicly released the Llama 2 family of large language models'\n",
    "inputs = tokenizer(sentence)\n",
    "tokenizer.decode(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b23e65-ef4d-410d-acc5-52f10c1b979c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db8144ef-7f8f-4a3f-b5bc-89899a04c7d9",
   "metadata": {},
   "source": [
    "# Generate Output\n",
    "\n",
    "- The output starts with our input: 'What is the language model, and how does it work?'. Observe that instead of answering our question, the model responds with more questions about the language models. Why so?\n",
    "\n",
    "    - Remember that we are exploring the base model and not the chat model. The base model was trained to predict the next word based on the vast set of Internet data, and the Internet is full of web pages listing Q&A or similar structures containing the list of questions. Following our input, the model generated an \"average Internet page\" starting from our question. So, the base model behaves as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2188e26a-5f4c-41b1-9777-8cabeae57a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁What', '▁is', '▁the', '▁language', '▁model', ',', '▁and', '▁how', '▁does', '▁it', '▁work', '?']\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "request = 'What is the language model, and how does it work?'\n",
    "print(tokenizer.tokenize(request))\n",
    "print(len(tokenizer.tokenize(request)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a14d81f6-23f1-4c4d-bf98-ef6717112deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,  1724,   338,   278,  4086,  1904, 29892,   322,   920,   947,\n",
      "           372,   664, 29973]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(request, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "print(len(inputs['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe79b3fa-fc96-451f-9cb7-8c5f2c985e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:649: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "inputs = inputs.to(model.device)\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ca1771b-3025-4d46-a1cb-c96ff8e6efe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  1724,   338,   278,  4086,  1904, 29892,   322,   920,   947,\n",
       "           372,   664, 29973,    13,    13, 29909,  4086,  1904,   338,   263,\n",
       "          1134,   310, 23116, 21082,   313, 23869, 29897,  1904,   393,   338,\n",
       "         16370,   373,   263,  2919,  8783,   310,  1426,   304,  8500,   278,\n",
       "          4188, 22342,   310,   263,  2183,  5665,   310,  3838,   470,  4890,\n",
       "         29889,   450,  1904,   508,   367,  1304,   363,   263, 12875,   310,\n",
       "          9595, 29892,  1316,   408,  4086, 13962, 29892,  1426, 19138,  2133,\n",
       "         29892,   322, 13563, 29890,  1862, 29889,    13,    13,  1576,  6996,\n",
       "          2969,  5742,   263,  4086,  1904,   338,   304,  7945,   263, 19677,\n",
       "          3564,   304,  8500,   278,  2446,  1734,   297,   263,  5665,   310,\n",
       "          1426,  2183,   278,  3517,  3838, 29889,   450,  3564, 24298,  1983,\n",
       "           304,   437,   445]], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e4b8bad-ae9d-4710-b7a6-6624c7c5c64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> What is the language model, and how does it work?\n",
      "\n",
      "A language model is a type of artificial intelligence (AI) model that is trained on a large dataset of text to predict the likelihood of a given sequence of words or characters. The model can be used for a variety of tasks, such as language translation, text summarization, and chatbots.\n",
      "\n",
      "The basic idea behind a language model is to train a neural network to predict the next word in a sequence of text given the previous words. The network learns to do this\n"
     ]
    }
   ],
   "source": [
    "response = tokenizer.decode(outputs[0])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50a59121-d092-4b8e-a79a-caa7c0a402e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa21431e-9a98-4fbf-a4b2-060b8b196508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
